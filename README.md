# LanguageModels_pre_W2V
Language Models that were precursor to modern word embeddings such as Word2Vec

* Before attempting the best-in-class Language models (pre-trained or otherwise) that involve seq2seq networks and transformers, I wanted to take baby steps to understand the fundamentals better
* That is the reason behind developing codes and notes for Language Models created before the Word2Vec era. 
