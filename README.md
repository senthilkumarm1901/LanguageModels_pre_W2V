**Statistical Markov Language Models** (based on counting of words), that were precursor to modern word embeddings such as Word2Vec, are discussed here. 

* Before attempting the best-in-class Language models (pre-trained or otherwise) that involve seq2seq networks and transformers, I wanted to take baby steps to understand the fundamentals better
* That is the reason behind developing codes and notes for Bigram and Trigram Markov Language Models.
* The below link has my notes on `Statistical Markov Language Models`: <br>
https://senthilkumarm1901.github.io/LanguageModels_pre_W2V/
