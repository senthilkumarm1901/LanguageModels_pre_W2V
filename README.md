# LanguageModels_pre_W2V
Statistical Markov Language Models (based on counting of words) that were precursor to modern word embeddings such as Word2Vec are discussed here. 

* Before attempting the best-in-class Language models (pre-trained or otherwise) that involve seq2seq networks and transformers, I wanted to take baby steps to understand the fundamentals better
* That is the reason behind developing codes and notes for Bigram and Trigram Markov Language Models. 
