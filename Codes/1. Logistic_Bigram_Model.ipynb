{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73c2386427883d98073da143b5bde084714d39f7"
   },
   "source": [
    "https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python <br>\n",
    "http://www.aclweb.org/anthology/E09-3003 <br>\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.6525&rep=rep1&type=pdf <br>\n",
    "https://www.udemy.com/natural-language-processing-with-deep-learning-in-python/learn/v4/questions/4267184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7afba0645cd701156bc9ad5c1a59b35d12e0fe00",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text = brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7932bde2f7ead95b799a6135bc3c9d37413cca31"
   },
   "outputs": [],
   "source": [
    "news_text_lowered=[[each_word.lower() for each_word in each_sent] for each_sent in news_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0cef17bb19713d3308077d4541ef4a8c1f26c30b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text_lowered_list=[each_word for each_sent in news_text_lowered for each_word in each_sent]\n",
    "Unique_words=list(set(news_text_lowered_list))\n",
    "V=len(Unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "916a5229f1551b2c1b8e064e609f2b5b7e2b8b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 238\n"
     ]
    }
   ],
   "source": [
    "#creating X (Input - last word) and Y (Target - current word) vectors\n",
    "\n",
    "def get_sentences_with_word2idx():\n",
    "    sentences = news_text_lowered[0:20]\n",
    "    indexed_sentences = []\n",
    "\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    print(\"Vocab size:\", i)\n",
    "    return indexed_sentences, word2idx\n",
    "\n",
    "sentences, word2idx = get_sentences_with_word2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "63a54c14d7a2bb006d128d7c88ad9e037ef3f2d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']\n",
    "\n",
    "# train a logistic model\n",
    "W = np.random.randn(V, V) / np.sqrt(V)\n",
    "\n",
    "lr = 1e-1\n",
    "losses = []\n",
    "def softmax(a):\n",
    "    a = a - a.max()\n",
    "    exp_a = np.exp(a)\n",
    "    return exp_a / exp_a.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "6252c019f6fff73780f392f0bedcab347d823030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "894ce820028fbb91c5482fa196cb68006f86c7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']\n",
    "each=0\n",
    "for sentence in sentences:\n",
    "    each+=1\n",
    "    print (each)\n",
    "    # convert sentence into one-hot encoded inputs and targets\n",
    "    sentence = [start_idx] + sentence + [end_idx]\n",
    "    n = len(sentence)\n",
    "    inputs = np.zeros((n - 1, V))\n",
    "    targets = np.zeros((n - 1, V))\n",
    "    inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "    targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "    # get output predictions\n",
    "    predictions = softmax(inputs.dot(W))\n",
    "    # do a gradient descent step\n",
    "    W = W - lr * inputs.T.dot(predictions - targets)\n",
    "    # keep track of the loss\n",
    "    loss = -np.sum(targets * np.log(predictions)) / (n - 1)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "698dccbc2b316755ff5a76d464d05d6ab6f3e97d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_bigram(new_sent):\n",
    "    new_sent=new_sent.lower()\n",
    "    new_words=new_sent.split()\n",
    "    new_words=[start_idx] + new_words + [end_idx]\n",
    "    new_words_index=[]\n",
    "    for each in new_words:\n",
    "        if each in word2idx.keys():\n",
    "            new_words_index.append(word2idx[each])\n",
    "        else:\n",
    "            new_words_index.append(999)\n",
    "    print (new_words_index)\n",
    "    n = len(new_words_index)\n",
    "    inputs_new = np.zeros((n - 1, V))\n",
    "    inputs_new[np.arange(n - 1), new_words_index[:n-1]] = 1\n",
    "    predictions = softmax(inputs_new.dot(W))\n",
    "    return predictions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c1c0123ab6035fcb4a11ab947e941422ea808862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "print (news_text_lowered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "08075f336d98737cf405750f6c4c7fe6a65d34cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999, 2, 3, 4, 6, 7, 999]\n",
      "6.0\n",
      "[999, 215, 999, 999, 138, 999, 999]\n",
      "6.0\n",
      "[999, 68, 69, 70, 71, 11, 72, 60, 999]\n",
      "8.0\n",
      "[999, 999, 999, 999, 999, 999]\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print (logistic_bigram(\"the fulton county jury said\"))\n",
    "print (logistic_bigram(\"some randomui statementio is thisp\"))\n",
    "print (logistic_bigram('only a relative handful of such reports'))\n",
    "print (logistic_bigram(\"ramdndomm fhlkh kjkel kjlk\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
