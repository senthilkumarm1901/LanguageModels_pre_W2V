{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Language Modeling\n",
    "## Table of Contents\n",
    "\n",
    "* [Introduction to Language Model and its Applications](#intro)\n",
    "* [n-gram Language Model](#ngram)\n",
    "* [Out of Vocabulary](#oov)\n",
    "* [Markov Assumption](#markov)\n",
    "* [Evaluation Metric: log_probability](#logprob)\n",
    "* [Evaluation Metric: perplexity](#perplexity)\n",
    "* [Application_in_text_classification](#classification)\n",
    "* [Application_in_language_generation](#text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "### Introduction to Language Model and its Applications\n",
    "\n",
    "\n",
    "Language Model: **A model of the probability of a sequence of words**\n",
    "\n",
    "A language model can assign probability to each possible next word. And also, help in assigning a probability to an entire sentence. \n",
    "\n",
    "#### Applications:\n",
    "\n",
    "* Predicting upcoming words or estimating probability of a phrase or sentence is useful in noisy, ambiguous text data sources \n",
    "* Speech Recognition: E.g.: P('recognize speech') >> P('wreck a nice beach')\n",
    "* Spelling Correction: E.g.: P('I have a gub') << P('I have a gun')\n",
    "* Machine Translation: E.g.: P('strong winds') > P('large winds')\n",
    "* Optical Character Recognition/ Handwriting Recognition\n",
    "* Autoreply Suggestions\n",
    "* Text Classification (discussed with python implementation of a simple N-gram model)\n",
    "* Text Generation (discussed this with Char-level and Word-level language models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ngram'></a>\n",
    "### N-gram Language Modelling\n",
    "\n",
    "\n",
    "**Sample Corpus**: <br>\n",
    "> This is **the house** that Jack built.<br>\n",
    "> This is **the** malt<br>\n",
    "> That lay in **the house** that Jack built.<br>\n",
    "> This is **the** rat,<br>\n",
    "> That ate **the** malt<br>\n",
    "> That lay in **the house** that Jack built.<br>\n",
    "> This is **the** cat,<br>\n",
    "> That killed **the** rat,<br>\n",
    "> That ate **the** malt<br>\n",
    "> That lay in **the house** that Jack built.<br>\n",
    "\n",
    "What is the probability of **house** occurring given **the** before it?\n",
    "\n",
    "Intuitively, we count,\n",
    "\n",
    "$$ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the above formula can be writen as $$ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have computed above is a **Bigram Language Model**:\n",
    "\n",
    "$$ Bigram\\:Model : { p\\,( W_t|W_{t-1} ) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do ngrams help us calculate the prob of a sentence?**:\n",
    "> Sentence, S = 'A B'<br>\n",
    "> We know that, probability of this sentence 'A B' as,<br>\n",
    "> P (S) = Prob (A before B) = P(A , B) = Joint Probability of A and B = **P(B|A)* P(A)**<br>\n",
    ">\n",
    "> Let us assume a three word sentence, S = 'A B C'<br>\n",
    "> P(S) = Prob (A before B before C ) = P (A , B , C)<br>\n",
    ">      = P(C | A , B) * P (A n B)<br>\n",
    ">      = P (C| A , B) * P(B | A) * P (A)<br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if there are more words, we could keep applying Conditional Probability and compute the prob of a sentence.\n",
    "The above rule is called **`Chain Rule of Probability`**\n",
    "$$ Prob \\ ({ A_n,\\ A_{n-1},\\ ....,A_1}) = Prob\\ ( {A_n \\ |\\ {A_{n-1},\\ ....,A_1}} ) \\  \\times \\  Prob\\ (A_{n-1},\\ ....,A_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With four vairables the chain rule of probability is: \n",
    "$$ Prob \\ ({ A_4,\\ A_3,\\ A_2,\\ A_1}) = Prob\\ ( {A_4 \\ |\\ {A_{3},\\ A_2,\\ A_1}} ) \\  \\times \\  Prob\\ ( {A_3 \\ |\\ {A_{2},\\ A_1}})\\ \\times \\ Prob\\ ({A_2 \\ |\\ A_1})\\ \\times \\ Prob\\ (A_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='oov'></a>\n",
    "### Out of Vocabulary\n",
    "\n",
    "Suppose we have a new sentence like the below one:<br>\n",
    "If we calculate the Probability of the below sentence, it would be zero<br>\n",
    "<br>\n",
    "> This is **the dog**,<br>\n",
    "> That scared the cat,<br>\n",
    "> That killed the rat,<br>\n",
    "> That ate the malt,<br>\n",
    "> That lay in the house that Jack built.<br>\n",
    "\n",
    "By our **chain rule of probabilities** where we keep multiplying probabilities,\n",
    "we would encounter $ P({dog \\over the}) = 0 $ , hence the overall probability of the above example sentence would be zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One way to avoid, Smoothing!\n",
    "\n",
    "$$ MLE: P({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ ) \\over Count\\ (\\ B\\ )} $$\n",
    "\n",
    "**Why is it called MLE?**\n",
    "* Suppose a bigram “the dog” occurs 5 times in 100 documents.\n",
    "* Given that we have this 100 document corpus (which the language model represents), the maximum likelihood of the bigram parameter “the dog” appearing in the text is 0.05\n",
    "\n",
    "**Add 1 Smoothing (Laplace Smoothing)**\n",
    "$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ 1 \\over Count\\ (\\ B\\ )\\ +\\ |V|} $$\n",
    "* We pretend that each unique bigram occurs once more than it actually did! \n",
    "* Since we have added 1 to each bigram, we have added |V| bigrams in total. Hence normalizing by adding |V| to the denominator \n",
    "\n",
    "\n",
    "**Add- $ \\delta $ Smoothing!**\n",
    "$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ \\delta \\over Count\\ (\\ B\\ )\\ +\\ \\delta * |V|} $$\n",
    "* $\\delta$ is any fraction such as 0.1, 0.05, etc.,\n",
    "* Unlike Add one smoothing, which reduces drastically the prob of high occurring words, this solves the zero prob issue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"markov\"> </a>\n",
    "### What is Markov Assumption?  \n",
    "\n",
    "So the probability of occurrence of a 4-word sentence, by markov assumption is: \n",
    "$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B,\\ C,\\ D}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C,\\ D}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n",
    "\n",
    "\n",
    "\n",
    "> “What I see NOW depends only on what I saw in the PREVIOUS step”\n",
    "\n",
    "$$ P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1}) = P(w_i,\\ |\\ {w_{i-1}}) $$\n",
    "\n",
    "Hence the probability of occurrence of a 4-word sentence with Markov Assumption is: \n",
    "$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n",
    "\n",
    "- What are its advantages?\n",
    " - Probability of an entire sentence could be very low but individual phrases could be more probable. <br>\n",
    "\tFor e.g.: <br>\n",
    "\t- Actual Data: “The quick fox jumps over the lazy dog”  \n",
    "\t- Probability of a new sentence: Prob(“The quick fox jumps over the lazy cat”) = 0 (though probable 'cat' is not there in vocab)\n",
    "\t- In Markov assumption (with additive smoothing), the above sentence will have a realistic probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"perplexity\"> </a>\n",
    "### Evaluation – Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perplexity is a measurement of how well a probability model predicts a sample. \n",
    "- It is used to compare probability models. \n",
    "- A low perplexity indicates the probability distribution is good at predicting the sample       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: \n",
    "- Perplexity is the inverse probability of  the test set, normalized by the number  of words. <br>\n",
    "Perplexity of test data = PP(test data) =\n",
    "$$ P(w_1,\\ w_2,\\ ....,\\ w_N)^{1 \\over N}  $$\n",
    "$$ $$ \n",
    "$$ {\\sqrt[N]{1 \\over P(w_1,\\ w_2,\\ ....,\\ w_N)}} $$\n",
    "$$ $$\n",
    "$$ {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1})}}}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the lower the perplexity, the better is the model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logprob\"> </a>\n",
    "### Evaluation - Log Probability\n",
    "- But multiply probabilities like these could end up giving you the result closer to zero. \n",
    " - For e.g.: P(D|C) = 0.1, P(C|B) = 0.07, P(B|A) = 0.05, P(A) = 0.2\n",
    " - P(A before B before C before D before E) = 0.00007 $-->$ too low or almost zero !\n",
    "\n",
    "- Logarithm to the rescue ! \n",
    " - Logarithm is a monotonically increasing function ! \n",
    " - Meaning: If P(E|D) > P(D|C), then log(P(E|D)) > log (P(D|C))\n",
    " - Also, log (A *B) = log (A) + log (B) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LogarithmChart](https://upload.wikimedia.org/wikipedia/commons/1/17/Binary_logarithm_plot_with_ticks.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) $$ \n",
    "- where N  is the number of words in the sentence. \n",
    "- Since log (probabilities) are always negative (see graph), shorter sentences will have higher probability of occurrence. \n",
    "\n",
    "**To normalize it**, \n",
    "$$ {1 \\over N} \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n",
    "\n",
    "**Higher the log probability value, better is the model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between Perplexity and Log-Probability metrics for a Bigram Markov Language Model\n",
    "\n",
    "$$ Perplexity\\ for\\ a\\ bigram\\ model = {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1}})}}}$$ \n",
    "\n",
    "Comparing with normalized log probability for a bigram model:  \n",
    "$$ LogProbability\\ for\\ a\\ bigram\\ model = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classification\"> </a>\n",
    "### Application in Text Classification\n",
    "\n",
    "In this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Bigram_Models--Text_Classification.ipynb), Markov Bigram model is implemented on a text classification problem. We can build a deterministic classifier where class is attributed to the class with highest log probability score.\n",
    "\n",
    "```python\n",
    "print(bigram_markov(\"economic growth slows down\"))\n",
    "```\n",
    "\n",
    "```\n",
    "# log probability score of different classes\n",
    "[('business', -3.9692388514802905),\n",
    " ('politics', -6.7677569438805945),\n",
    " ('tech', -8.341093396600021),\n",
    " ('sport', -9.000286606679415),\n",
    " ('entertainment', -9.09610496174359)]\n",
    "\n",
    "Class attributed: 'business'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "print(bigram_markov(\"prime minister has gone to the US on an official trip\"))\n",
    "```\n",
    "\n",
    "```\n",
    "# log probability score of different classes\n",
    "[('politics', -3.5746871267381852),\n",
    " ('business', -7.440089097987748),\n",
    " ('tech', -10.252850263607193),\n",
    " ('sport', -11.981405120960808),\n",
    " ('entertainment', -12.124273481509913)]\n",
    " \n",
    " Class attributed: 'politics'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text_generation\"> </a>\n",
    "### Application in Text Generation\n",
    "\n",
    "In this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Char-level%20N-gram%20Language%20Generation%20Model.ipynb), a char-level LM, using trigrams and bigrams of characters, was used to generate dianosour names.\n",
    "\n",
    "```python\n",
    ">> print(names.iloc[:,0].head())\n",
    "\n",
    "# Names of Dianosours used as input\n",
    "0     Aachenosaurus\n",
    "1          Aardonyx\n",
    "2    Abdallahsaurus\n",
    "3       Abelisaurus\n",
    "4     Abrictosaurus\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    ">> char_level_trigram_markov_model(names)\n",
    "\n",
    "# Names of Dianosours generated as output\n",
    "broplisaurus\n",
    "prorkhorpsaurus\n",
    "jintagasaurus\n",
    "carsaurus\n",
    "gapophongasaurus\n",
    "teglangsaurus\n",
    "borudomachsaurus\n",
    "zheisaurus\n",
    "horachillisaurus\n",
    "aveiancsaurus\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> References:\n",
    ">\n",
    "> Wikipedia <br>\n",
    "> [Udemy Course on Deep NLP by Lazy Programmer](https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/) <br>\n",
    "> [NLP Course on Coursera by National Research University Higher School of Economics](https://www.coursera.org/learn/language-processing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
